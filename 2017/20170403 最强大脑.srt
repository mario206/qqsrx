
1
00:00:11.590 --> 00:00:13.110
说话人 1: 锵锵三人行

2
00:00:13.190 --> 00:00:17.750
说话人 1: 唉呦，我感觉跟这个两位最强大脑聊天真是显得自己很蠢

3
00:00:17.750 --> 00:00:31.130
说话人 1: 哈哈哈，郝景芳老师，你的这个科幻小说北京折叠，我就说当年的我们刚知道他的时候都说是清华美女什么的最强大脑

4
00:00:31.250 --> 00:00:34.530
说话人 1: 怎么我觉得你自己还说你自个在清华的时候是学渣

5
00:00:34.530 --> 00:00:34.770
说话人 1: 还

6
00:00:35.670 --> 00:00:48.950
说话人 2: 对，在清华里面可能大部分人都会有这个感觉，就是不管是自己学习成绩怎样，都会觉得周围的人都比自己还要更厉害一些，所以这个心情是就是比较

7
00:00:48.950 --> 00:00:49.510
说话人 1: 真实的

8
00:00:49.590 --> 00:00:54.230
说话人 1: 你都得雨果奖了，比你还厉害，那得什么爱因斯坦奖了，我又没有

9
00:00:54.230 --> 00:00:58.750
说话人 2: 在清华里面就得雨果奖，这不是都已经毕业了好多年以后的事情吗

10
00:00:58.750 --> 00:00:59.110
说话人 2: 不是

11
00:00:59.460 --> 00:01:03.580
说话人 1: 靠写小说得奖，正说明理科教育是学渣是吗

12
00:01:04.700 --> 00:01:10.860
说话人 2: 就但因为确实学校里面各种各样哪个方向非常厉害的人都很有

13
00:01:11.580 --> 00:01:26.900
说话人 2: 我们系里面就有那种各个科目都考到九十几分 95 分以上，然后还会吹长笛了，然后就是写，随便写一个英文的 paper 就国网国外都能发到最好的杂志上

14
00:01:27.020 --> 00:01:32.420
说话人 2: 就是人外有人，天外有天，我觉得在这样的环境里面锻炼一下也好

15
00:01:32.780 --> 00:01:39.770
说话人 1: 唉，这个真的是，吴军老师，您说就是咱们那天谈的这个问题哈，在这里边有个很残酷的现实

16
00:01:39.770 --> 00:01:43.970
说话人 1: 嗯，你看我身边有一些朋友，我就有时候感觉到

17
00:01:44.010 --> 00:02:04.760
说话人 1: 嗯，怎么说他们是智力很高，但是他们经常会流露出一种对这个低智力的人的那种烈士，就是脑残，这是很残酷，就是人到底是不是平等的，至少咱就说这个智力，唉，就有的就是脑残能这么

18
00:02:04.760 --> 00:02:06.440
说话人 3: 智商有差异

19
00:02:06.440 --> 00:02:13.720
说话人 3: 这个肯定的，这个但是就是说我觉得每个人都有自己的天分，所以你一定要

20
00:02:13.760 --> 00:02:15.360
说话人 1: 蠢，人也有天分

21
00:02:15.400 --> 00:02:17.000
说话人 3: 有保护期，有他的天分

22
00:02:17.000 --> 00:02:30.670
说话人 3: 我跟你讲，我在中学时候遇到一个人哈，就是给有一个词叫白痴天才，有一种这种就像在蚁人似的，他特别就是这智力完全有问题，就是傻子

23
00:02:30.670 --> 00:03:04.590
说话人 3: 但是他有一个本领，我们都很惊讶，就是他算这个三位数乘四位数的这个乘法，你告诉他，把这数字告诉他，比如说 378 * 1256，然后他马上就把答案说给你了，不用想的，说给你了，然后你在那个纸上写或者地上画，我，唉，你发现他是算的，对的，是四位数乘四位数，什么三位四数乘三，不知道他怎么算的，我至今我搞不懂他是怎么算的，所以看的那个电影就是那个达斯汀霍夫曼的那个与人哪，我就这个确实是有这样的一些人

24
00:03:05.400 --> 00:03:07.640
说话人 1: 对，那你要这么说，您讲我

25
00:03:07.680 --> 00:03:25.040
说话人 2: 我是觉得就是你要看到某些人觉得其他人是脑残的话，其实比较好的一个办法是你要是听到另外一边人可能也觉得这边是脑残，就是两边在吵架的时候都会觉得对方是脑残，这个其实跟真的聪明和笨没有任何的关系

26
00:03:25.040 --> 00:03:25.520
说话人 2: 对，是

27
00:03:25.520 --> 00:03:27.120
说话人 3: 偏急就是偏激

28
00:03:27.120 --> 00:03:34.310
说话人 2: 就是喜欢觉得我的我，我说的就是对的，我说的无论如何怎么都是对的，你说的无论如何怎么都是不对的

29
00:03:34.470 --> 00:03:42.830
说话人 2: 然后在这种情况下就有一种我自己智商比你高的幻觉，然后其实真的两边可能都是这么想的

30
00:03:43.470 --> 00:03:53.950
说话人 1: 这个也有可能，你比如说看从哪方面说，你好比说爱因斯坦，他能想出相对论，但是他未必知道他们家酱油瓶子放哪，这对吧

31
00:03:53.950 --> 00:03:55.990
说话人 1: 看你怎么来衡量这个智能

32
00:03:56.470 --> 00:04:01.550
说话人 1: 唉，那现在您说这个人工智能，我们说的这个智能是什么智能

33
00:04:02.040 --> 00:04:05.880
说话人 3: 不是我，这是两回事，就是我们说人工智能和人的智能其实是两回事儿

34
00:04:06.280 --> 00:04:13.480
说话人 3: 人工智能就是说它从效果上来解决人，原来由人脑解决了问题，解决了比例还好

35
00:04:13.690 --> 00:04:21.880
说话人 3: 比如说我们现在说识别人的语音写文章，包括那个华尔约日报和那个和纽约时报，好多是机器在写的，也很漂亮

36
00:04:21.880 --> 00:04:22.160
说话人 3: 真的

37
00:04:22.160 --> 00:04:22.320
说话人 1: 吗

38
00:04:22.320 --> 00:04:22.440
说话人 1: 对

39
00:04:22.480 --> 00:04:30.090
说话人 3: 对对，已经是这样的，而且前一阵北大搞出了一个样本，我不知道你看了没有，就是写了一篇小样文，写的蛮漂亮的

40
00:04:30.090 --> 00:04:35.370
说话人 3: 写诗，对，写的蛮漂亮，就是说但是他的思维方式和我们人是不一样的

41
00:04:35.660 --> 00:04:42.740
说话人 3: 就是前面他其实讲了，刚才那个警方讲了，就是说人的思维方式很复杂了，跟我们的比如说那个激素分泌都是有关的

42
00:04:42.980 --> 00:04:45.900
说话人 3: 机器思维方式其实就是一个算，就是个算

43
00:04:45.900 --> 00:04:58.440
说话人 3: 包括那个前一阵烈酒段来说，这个讲围棋机器不知道它在下棋，但是人你是有很强的目的性，你知道你是在干什么，所以这两个智力其实是两回事

44
00:04:58.480 --> 00:05:02.520
说话人 3: 但是大家对比，它只是说我们同样做一件事儿，谁做得更好

45
00:05:02.750 --> 00:05:06.420
说话人 3: 今天来讲很多地方机器能比人做得好，嗯，仅此而已

46
00:05:06.420 --> 00:05:09.220
说话人 3: 并不是说它获得了我们人类的智能，这是两

47
00:05:09.300 --> 00:05:18.990
说话人 2: 那个就是其实在那个斯坦诺维奇有一个书叫超越智商，嗯，其中就是分说有算法那个智力，然后以及反省的

48
00:05:18.990 --> 00:05:20.870
说话人 2: 嗯，这个智商，然后算法智商呢

49
00:05:20.870 --> 00:05:26.150
说话人 2: 就是我告诉你你要解决这个问题，然后看你怎么能解决的最好，然后反省伤呢

50
00:05:26.150 --> 00:05:29.110
说话人 2: 其实是要想我为什么要解决这问题

51
00:05:29.110 --> 00:05:30.510
说话人 2: 我要不要解决这问题呢

52
00:05:30.670 --> 00:05:39.350
说话人 2: 所以你要是让这个目前的人工智能，你让它下围棋，它就下的特别好，它算法智力超群，这个现在是目前没问题，它肯定是超越人类了

53
00:05:39.450 --> 00:05:46.240
说话人 2: 但是就是什么时候他到了那反省之力，就是你让他下围棋，他要先衡量一下我下不下这个围棋，我为什么要下围棋

54
00:05:46.360 --> 00:05:47.520
说话人 2: 为什么要听你的

55
00:05:47.560 --> 00:05:49.120
说话人 2: 我是听你的还是听他的

56
00:05:49.280 --> 00:05:51.280
说话人 2: 然后我干这件事到底对不对

57
00:05:51.280 --> 00:05:58.120
说话人 2: 我们就是他什么时候开始这样了，就会有一点点接近人，但是这个反省智力是如何产生的

58
00:05:58.280 --> 00:06:04.000
说话人 2: 就是人的这种目的、价值观意义，这个是很复杂的事情，就又不一样

59
00:06:04.000 --> 00:06:04.080
说话人 1: 了

60
00:06:04.080 --> 00:06:15.710
说话人 1: 对，你说这个就有意思，就是说那人的这个所谓价值观该不该做这件事儿实际上会不会最终发现也只不过是更复杂的算法

61
00:06:15.710 --> 00:06:17.830
说话人 2: 对，进化心理学的话会感受

62
00:06:17.830 --> 00:06:30.550
说话人 1: 你比如说我给你举个例子，就是我有个事印象很深，就是卡夫卡，有个这个著名小说家，卡夫卡这个人非常有意思，这卡夫卡口才也非常好，实际他说话都是这个二律被反的

63
00:06:30.590 --> 00:06:39.930
说话人 1: 我对他印象很深，他跟人聊天，比如说据说卡夫卡的临终遗言哪就是他太疼了，他就跟那个医生说说杀死我，不然你就是杀人犯

64
00:06:40.170 --> 00:06:42.490
说话人 1: 你看他的语言就是杀死我，不然你就是杀人犯

65
00:06:42.890 --> 00:06:43.770
说话人 1: 卡夫卡

66
00:06:43.850 --> 00:06:50.210
说话人 1: 他曾经犹豫过两三年时间，就是他要不要跟一个女的结婚

67
00:06:50.210 --> 00:07:08.000
说话人 1: 嗯，在这两三年之间他跟这个女的无数次长谈，谈 10 个小时，然后他就在纸上写出所有的我要跟他结婚的理由，所有的不要跟他结婚的理由，最终还是没有做不出决定

68
00:07:08.240 --> 00:07:10.440
说话人 1: 但是这件事你看像是个隐喻

69
00:07:10.480 --> 00:07:15.040
说话人 1: 嗯，你说他，你说这人的自由意志，其实他不也是算嘛

70
00:07:15.080 --> 00:07:20.030
说话人 1: 对，包括我对你有多少感情，对，也是一个参数嘛，也是我衡量的

71
00:07:20.150 --> 00:07:25.390
说话人 1: 唉，你有多漂亮，你有多少钱，我都有多爱你，这不就是个综合算法

72
00:07:25.390 --> 00:07:25.430
说话人 3: 吗

73
00:07:25.430 --> 00:07:28.070
说话人 3: 这我觉得是在理性的时候

74
00:07:28.070 --> 00:07:39.780
说话人 3: 是这样，真是陷入感情的时候，你一开始那个什么那个生产腺素分泌比较多的时候，或者哆胺分泌比较多的时候，它不是这样子，它是完全受化学物质控制的，这是

75
00:07:40.020 --> 00:07:42.290
说话人 2: 那个就是这个问题

76
00:07:42.290 --> 00:07:52.890
说话人 2: 反正进化心理学确实像您说的，他是那个全都是为了这个物种进化，然后您喜欢这个人其实是背后的理由是，就是都是为了对你在

77
00:07:52.890 --> 00:07:55.010
说话人 1: 你为什么多巴胺出来你高兴了

78
00:07:55.010 --> 00:07:56.570
说话人 1: 因为你发现了好的繁殖对象

79
00:07:56.570 --> 00:07:57.250
说话人 1: 对对对

80
00:07:57.250 --> 00:08:28.090
说话人 2: 就是进化心理学是这么说的，但是人和这种完全受这个基因控制的还有一点点不一样的，就是说人有一种会希望这个决定是我做的那种本能就是说哪怕这个人是一个很好的繁衍对象，那种说这个是我找的人和这个是被安排在我面前的人的，就是这两者对于人来讲是有差异的，就说这个决定是有差异，对，这个决定是我做的，就这件事对于人就很有意义

81
00:08:28.210 --> 00:08:29.810
说话人 2: 就举一个什么样的例子呢

82
00:08:29.810 --> 00:08:35.460
说话人 2: 你比如说你像手机Siri，你让他去做一个指令他，他就给你去干了

83
00:08:35.620 --> 00:08:57.760
说话人 2: 但是你要像一个一岁的小孩说你过来他不要，然后待会你说你走，让他不要，就说连一岁、两岁的小孩他都有一种说这件事是要是我自己想干的就OK，别人让我干的就不OK，但是就是这种，那个说这个事是我自己要做决定的，对，这个情绪其实是一个，你

84
00:08:57.760 --> 00:08:58.480
说话人 1: 怎么嗯

85
00:08:58.480 --> 00:09:00.240
说话人 1: 知道你不是自欺欺人呢

86
00:09:00.550 --> 00:09:15.870
说话人 1: 就是就是你，你这不是有个电视美剧叫西部世界，我在那里里边有一幕印象很深，就一个女的机器人，对，一下子把自己升级到最高智能了，然后他说我要逃离这个牢笼，逃离这个园子

87
00:09:15.950 --> 00:09:32.650
说话人 1: 结果后来另一个工程师就说，连你逃离的这个意志，嗯，也是最早以前你的发明者之一，嗯，写进你的头脑里的一个指令，他就觉得很困惑，说不会，这是我自由意志，我是要逃出去的

88
00:09:32.930 --> 00:09:38.370
说话人 1: 但是这也是你脑子里的一个程式，一个程序，你又怎么知道不是这样的

89
00:09:38.710 --> 00:09:50.600
说话人 3: 这个所谓自由意识的就是刚才讲，您讲一开始讲那本书，那个未来监视里就讲这个，这实际上就是可能是我们这个社会产生结果施加在人身上

90
00:09:50.600 --> 00:09:53.880
说话人 3: 然后他有一个自欺欺人的一个感觉，对

91
00:09:54.080 --> 00:10:12.310
说话人 2: 那个，对，就是这个问题，其实是一个很古老的 2000 年，大家都在讨论到底有没有自由意志，然后别人以前就是讨论神，但是现在我们就讨论一个心神，就是人工智能嘛，那个或者是我们讨论说整个都是一个宇宙大的算法，全都是以社会的影响

92
00:10:12.310 --> 00:10:24.350
说话人 2: 对对对，但是这个问题就在于说人，你从你主观感觉你到底有没有可能我在这一刻就决定我下一刻是往哪个方向走，就是不一样

93
00:10:24.350 --> 00:10:34.410
说话人 2: 比如说你这课决定你接下来要不要接我这句话，你到底有没有可能说是做一个不一样的决定，还是说一切都是宇宙算好了

94
00:10:34.410 --> 00:10:37.650
说话人 2: 说你下一分钟就是要说话，还是你下一分钟不说话

95
00:10:37.850 --> 00:10:51.590
说话人 2: 所以呢，这个悖论就在于说哪怕我们可以客观的觉得说噢，可都一切都是被决定好了，但是我们在主观上每一分每一秒都仍然觉得我这一刻可以这样，或者下一刻可以那样不一样

96
00:10:51.790 --> 00:11:11.410
说话人 2: 所以那个投斯托USG，那个群魔，那个主人公，他就是想要证明说我自己有自由意志，他就为了这个没有任何理由，他娶了一个，那个就是马路边的一个很穷的一个女性，然后就别人都在一直在猜测他有什么阴谋诡计，或者是这个里面有什么特殊的理由

97
00:11:11.530 --> 00:11:16.990
说话人 2: 是仇恨呐，还爱恨都没有，他就是想证明我有自由意志，他就他就娶了

98
00:11:16.990 --> 00:11:18.110
说话人 1: 那个你，他就是作吗

99
00:11:18.630 --> 00:11:21.150
说话人 1: 没事，no，作， no die，哈哈哈

100
00:11:21.150 --> 00:11:25.190
说话人 2: 当然了，反正你可以说这个这些也都是被写好的

101
00:11:25.190 --> 00:11:27.310
说话人 1: 他有一种三人行广告之后见

102
00:11:57.330 --> 00:12:07.410
说话人 1: 那那个吴军老师，您看未来简史这本书里的有个思想，就是说所有能够用算法描述的行为，对人工智能最终必将超越人类

103
00:12:07.450 --> 00:12:13.040
说话人 1: 对，而还有一句话就是说本质上宇宙的一切都是一种算法

104
00:12:13.520 --> 00:12:14.960
说话人 1: 这第二个您同意吗

105
00:12:14.960 --> 00:12:36.420
说话人 3: 这个我是这么想的，就是说这佛教都是一种算法，就是我看那个就是他跟我有一直脑子里在想的一件事，我觉得有一点相似处，就是说我一直说这个宇宙产生它有背后的很多规律，当然你可以认为这规律是一个算法，因为这个至少从我们现在人类认知好多现象是无法解释的

106
00:12:36.420 --> 00:12:47.140
说话人 3: 所以你必须认为它就是说它在宇宙诞生的最早的那一刹那，特别短时间里是没物理学的，所有的规律是不成立的，后来就产生了这些物理学的规律

107
00:12:47.440 --> 00:12:54.360
说话人 3: 那么比如说我们这个万有引力，就是说这个里头为什么吸引的程度是这么多

108
00:12:54.680 --> 00:12:56.280
说话人 3: 那不是更大或者更小

109
00:12:56.320 --> 00:12:58.640
说话人 3: 对，就是说这是由谁来控制的

110
00:12:59.410 --> 00:13:06.130
说话人 3: 那么实际上，比如说我们现在知道，实际上宇宙中代有四种力，就是作用力

111
00:13:06.130 --> 00:13:11.270
说话人 3: 当然后来发现希克斯力里海人是第五种，一种是重力，就是万有引力

112
00:13:11.270 --> 00:13:14.270
说话人 3: 我们大家都知道一种电磁力，大部分人也知道

113
00:13:14.550 --> 00:13:16.670
说话人 3: 还有一种叫强力，一种叫弱力

114
00:13:17.030 --> 00:13:23.390
说话人 3: 强力就是说我们这个原子核能够形成弱力，就是说我们由这个会裂变

115
00:13:23.790 --> 00:13:31.310
说话人 3: 这四种力度有参数，彼此之间这个参数要错出一点点来讲，我们的宇宙就不是今天这宇宙了

116
00:13:31.670 --> 00:13:33.670
说话人 3: 比如错差出多少来了

117
00:13:33.670 --> 00:13:45.720
说话人 3: 大概是这样的，就是我们地球上有多少个沙子我忘了，反正地球上那么多沙子，差出一颗来，这个宇宙要么就灰飞烟灭了，要么就缩成一团了

118
00:13:46.000 --> 00:13:50.320
说话人 3: 我的天哪，当然是，所以说就是为什么他如此的精准

119
00:13:50.600 --> 00:13:54.320
说话人 3: 对，所以为什么很多物理学家到后来姓上帝的事，这原因他实在解释不了

120
00:13:54.320 --> 00:14:26.000
说话人 3: 他说，噢，有一个高傲的，这个创造了这些规律，那么从一开始演化，实际上这个数据就存在，说这个一开始是说就像物理学的这些定律之间的那些参数，就是一些数据，然后慢慢它就会演变出我们的这个生有生命的物种，你可以叫它核心是DNA，是遗传物质，那某一代实际上是你这个 DNA 这个密码，那也是一个数据，它的不断

121
00:14:26.400 --> 00:14:28.880
说话人 3: 对，直到传到我们

122
00:14:29.040 --> 00:14:43.640
说话人 3: 嗯，就是人类出现，或者说甚至说智人出现以后，它有一个语言和这个能够书写的能力，文字又多出了一个新的知识的传播方式，以前只能传播，只能靠 DNA 来传播，往后带

123
00:14:43.640 --> 00:15:05.340
说话人 3: 对，这东西死了以后，慢慢的哪个 DNA 变异了，它，那么以后就也是来讲，总体上来说女只有两个都，一个是能能量，哎，还有一个是能量，就这本质上就这两个东西

124
00:15:05.500 --> 00:15:05.860
说话人 3: 对

125
00:15:06.500 --> 00:15:10.020
说话人 1: 那您看就是好像人类这个认知啊

126
00:15:10.020 --> 00:15:16.710
说话人 1: 对，发生了一个过程，最早咱们以神，咱们觉得神是这个控制一切，至高无上

127
00:15:16.710 --> 00:15:20.070
说话人 1: 对，最后有人说，那你采不是说上帝已死吗

128
00:15:20.110 --> 00:15:21.270
说话人 1: 上帝死了，上帝已死呢

129
00:15:21.270 --> 00:15:23.030
说话人 1: 那就到了这个以人为中心了

130
00:15:23.030 --> 00:15:27.280
说话人 1: 现在咱们考虑任何问题，就是说人是一家独大，对吧

131
00:15:27.280 --> 00:15:31.040
说话人 1: 我们是地球的主人，所有的东西万物为我所用，对吧

132
00:15:31.280 --> 00:15:36.920
说话人 1: 那么这人就又出于幺蛾子，就是他弄现在又出来了一个数据

133
00:15:37.140 --> 00:15:39.890
说话人 1: 像吴军老师，您现在提出一个词叫数据

134
00:15:39.890 --> 00:15:49.890
说话人 3: 主义，这是有些其他人我倒不完全觉得是数据主义，当时就是说我就是说信息，我就觉得信息和数据基本上划等号了

135
00:15:50.260 --> 00:15:55.570
说话人 3: 信息和能量，我觉得这两还一个是能量这两方面的一个作用

136
00:15:55.570 --> 00:16:07.530
说话人 3: 当然还有一条，第三条我觉得很重要，就是说物理学有一个测不准原理，很多人就是认为说人的意识来自于一个测不准，就是我们自我的意识就测不准

137
00:16:07.570 --> 00:16:08.250
说话人 3: 讲什么呢

138
00:16:08.250 --> 00:16:12.530
说话人 3: 就是说我们说过去，说上爱因斯坦，说上帝不掷色子

139
00:16:12.690 --> 00:16:14.850
说话人 3: 对，那波尔的意思就是上帝也掷色子

140
00:16:14.850 --> 00:16:19.320
说话人 3: 因为你那个就有一个悖论，叫薛定谔的猫吗

141
00:16:19.320 --> 00:16:24.160
说话人 3: 就是说他是死是活，你看他一眼他就知道了，不看他不知道是什么

142
00:16:24.160 --> 00:16:31.250
说话人 3: 对对对，所以这个意识也是这样，就是说你今天不去做决定，说你比如娶不娶她，不做这决定时候是不知道的

143
00:16:31.490 --> 00:16:39.510
说话人 3: 你一定说我自己要有一个我自己今天为自己做一次主导，导致了这个结果的产生，否则是没有这个结果

144
00:16:39.510 --> 00:16:39.670
说话人 1: 的

145
00:16:39.990 --> 00:16:52.940
说话人 1: 反正就是这里头好像有一个，我觉得现代科学还没有发现，就是说按说人的一切行为就像西部世界里边那个智能机器人一样，都是通过一个算法可以进行完

146
00:16:53.020 --> 00:17:07.750
说话人 1: 对，但是唯一一点就是说人为什么会有这种自我的这种生气，嗯，愤怒，按说你没这些情绪，你的条件反射也让你知道见到老虎你要跑，当然这雅戈尔动物园那个门没跑，是吧

147
00:17:07.790 --> 00:17:09.510
说话人 1: 那玩意也挺神，对吧

148
00:17:09.710 --> 00:17:11.270
说话人 1: 它也是一种算法门票

149
00:17:11.270 --> 00:17:14.070
说话人 1: 对对对， 130 块钱门票，他也算错了

150
00:17:14.070 --> 00:17:14.310
说话人 1: 对

151
00:17:14.520 --> 00:17:29.960
说话人 2: 那个就是像吴老师刚才说的那个就是量子力学的那测不准原理，那个实际上就是你可以包括量子力学也是这个东西是确定的，但并不是决定论的，就是它这个薛定论方程是很确定的

152
00:17:30.080 --> 00:17:33.160
说话人 2: 然后有了初始我们可以算出一个大概率的一个

153
00:17:33.200 --> 00:17:38.120
说话人 2: 对，那个，但是我们算的这个东西本身是一个概率性的一个概率分布的

154
00:17:38.240 --> 00:17:49.030
说话人 2: 就比如说我知道你是在这里出生的，你是在这样长大的，你这个过程我觉得你大概率可能不会是一个杀人犯，然后但是就是一切都是有概率的

155
00:17:49.310 --> 00:17:54.310
说话人 2: 然后只不过我们能知道的是你做这件事的概率有多大，那件事的概率有多大

156
00:17:54.470 --> 00:17:55.270
说话人 2: 所以呢

157
00:17:55.270 --> 00:18:04.070
说话人 2: 那个哪怕是我们都是数据，都是算法，把你的每一分、每一秒、每一个分子都算出来，我们可能最后也只能算出来一个概率分布

158
00:18:04.230 --> 00:18:07.590
说话人 2: 所以你真正做一件事的话还是会有一个随意性

159
00:18:07.630 --> 00:18:08.660
说话人 2: 对，当时会

160
00:18:08.660 --> 00:18:09.700
说话人 1: 确定

161
00:18:09.740 --> 00:18:19.860
说话人 1: 但是就说现在你看咱们可以聊一些比较实际的问题了，就是说这个概率似乎有的时候会表现出啊越来这个相差越大了

162
00:18:20.060 --> 00:18:23.460
说话人 1: 比方说就是您的有一次演讲里，您不也提到吗

163
00:18:23.580 --> 00:18:37.900
说话人 1: 就是你到未来的这个世界， 2% 的人能够脱颖而出人头地， 98% 的人可能有可能被淘汰，然后连这个去年人民日报都发表评论，就是说批评这个阶层固化

164
00:18:37.940 --> 00:18:45.870
说话人 1: 嗯，就是好像有一句话，就是说为什么我今天拼命努力自强不息，但是我还是一点都没机会

165
00:18:45.910 --> 00:18:51.270
说话人 1: 就是现在你说这个概率，那你知道 80 年代中后期，嗯，就是

166
00:18:51.270 --> 00:18:52.990
说话人 1: 唉，你们上大学那个年代

167
00:18:52.990 --> 00:18:56.790
说话人 1: 对，那个年代表现出就是贫家寒门子弟

168
00:18:56.790 --> 00:18:59.670
说话人 1: 嗯，确实跃身精英阶层就上了阶层了

169
00:18:59.710 --> 00:19:02.630
说话人 1: 对，是很大概率，对，但是今天你知道吗

170
00:19:02.630 --> 00:19:03.630
说话人 1: 他们就发现呢

171
00:19:04.030 --> 00:19:15.240
说话人 1: 百，好像我看到一个数据，就是上这个精英大学的，好像是说 1.7% 的精英家庭，它占据了上好大学的 40% 的

172
00:19:15.440 --> 00:19:21.920
说话人 1: 这就有名牌，北大、清华这种名牌大学就是越来越多的，考上好大学的是出身于经营家庭的子弟

173
00:19:22.040 --> 00:19:35.740
说话人 2: 确实就是这个，是因为我现在工作的地方中国发展研究基金会，我们做的就是那种贫困儿童项目，我们对这个最了解，就实际上人的在这个两岁之前的大脑发展是会非常非常重要

174
00:19:35.900 --> 00:19:43.100
说话人 2: 你两岁之前的大脑发展程度，你受到的这样的那个启蒙的程度，它是影响到你长大以后的智商的

175
00:19:43.300 --> 00:20:20.710
说话人 2: 就是对，他，不是说对，他是那个你两岁以前在这种富有的家庭，他接触到很多书、玩具，然后有很多人逗他，这个小孩子不管他做出什么反应，然后父母家长都在边上跟他说话说，噢，好棒啊等等，就这样的话，这些所有这些都是大脑，他在拼命的发展，他的每时每刻都有很多神经元在发展，但是那个贫困地区的孩子，尤其是一些留守的儿童，他连一个人跟他说话都没有，他的营养也不够，他的整个的这种智力的都不够，所以他的那个可能在大脑最开始的这个发展就已经有差距了

176
00:20:20.750 --> 00:20:25.590
说话人 2: 那你从 6 岁上学的时候就是这个差距就已经有了

177
00:20:25.590 --> 00:20:29.350
说话人 2: 哎，对，所以这个是一个，这个东西其实是影响很大的

178
00:20:29.350 --> 00:20:29.670
说话人 2: 这是一个

179
00:20:29.670 --> 00:20:35.370
说话人 3: 但是另外一个我觉得还是就是说跟家庭的那个家长的见识，我觉得很有关系

180
00:20:35.370 --> 00:21:27.680
说话人 3: 就我发现就是说我们中国老说一句话，就说不能输在起跑线上，我发现这个起跑线最大差距是父母的见识，有些这父母很有见识，说这孩子就给他一个很大的发展空间，有些他只会模仿其他的父母，那么这个孩子就很受约束，他说那个是有数据支持的，刚才那个警方说这件事因为在美国，他们美国因为得分层分得很清楚，所以你到那个，比如说贫门窟去做调查，他们就发现比美国因为大概有 70% 的黑人的家庭是，就非洲裔的家庭，是这个单亲家庭、单亲母亲，然后平时就没有爸爸跟他说话，所以他 4 岁以前他们要少说可能上千万的字，这下就差远了，听到就是少听到上千万的字

181
00:21:27.720 --> 00:21:34.890
说话人 1: 对，所以你说真正可怕的不是说富的越富，穷的越穷，而是聪明的越聪明，蠢的越蠢，这怎么办

182
00:21:35.010 --> 00:21:46.660
说话人 1: 但是你看，唉，咱们有人讲这特朗普当上你民主，嗯，那最后你要把我们这个寒门子弟都搞准了，我们还可以给你玩选票，嗯，对吧

183
00:21:46.660 --> 00:21:47.860
说话人 1: 我们给你颠覆了

184
00:21:47.940 --> 00:21:53.220
说话人 2: 但问题是特朗普也不能代表这就，他也未必能够帮得了这些寒门子弟

185
00:21:53.340 --> 00:21:54.610
说话人 1: 他不是蠢，他是疯

186
00:21:54.610 --> 00:22:01.130
说话人 3: 哈哈哈，我觉得特朗普是这样子，他和希拉里就是说，嗯，因为投票的还是穷人占多数

187
00:22:01.370 --> 00:22:03.650
说话人 3: 嗯，上层的不去管他

188
00:22:03.650 --> 00:22:12.290
说话人 3: 实际上支持特朗普的就是年薪5万以上的人，支持特朗普反而多一点，他们都有好多穷人支持特朗普

189
00:22:12.290 --> 00:22:20.300
说话人 3: 是这样的人是说我今天穷，但是我希望有个机会还愿意工作，改变我的状态

190
00:22:20.860 --> 00:22:24.500
说话人 3: 支持希拉里的说我今天穷，我就应该被养着

191
00:22:24.780 --> 00:22:27.620
说话人 3: 说穿了是这样，两拨人在决定噢

192
00:22:27.660 --> 00:22:30.620
说话人 1: 咱们先去下广告，锵锵三人行，广告之后见

193
00:22:45.880 --> 00:22:55.800
说话人 1: 最后我想问你们两位未来学家一个问题，就你记得梁树明老人过去一会书叫梁树明老人一脑门的官司，这个世界会好吗

194
00:22:56.350 --> 00:22:58.790
说话人 1: 展望未来的这个科技什么智能

195
00:22:59.030 --> 00:23:00.230
说话人 1: 你们怎么回答这个

196
00:23:00.230 --> 00:23:01.710
说话人 2: 问题

197
00:23:01.710 --> 00:23:08.590
说话人 2: 我写小说的话，我通常都会比较悲观主义，因为一个小说你写的悲观主义一点觉得比较有张力

198
00:23:08.910 --> 00:23:26.060
说话人 2: 但是我自己，其实从我个人而言，对于科技和人类未来，我是相对比较乐观的，包括这个科技科技的产生带来的这种失业，包括这个对环境的影响，包括人类未来的发展方向，我相对还是蛮乐观的

199
00:23:26.260 --> 00:23:35.310
说话人 2: 因为实际上你要是真正看我们过去的这一百年、二百年，我们这几千年人类的这个技术发展实际上是翻天覆地的

200
00:23:35.310 --> 00:23:40.550
说话人 2: 对，那我们从现在跟四千年、 50005000 年，还大家还都住山洞呢

201
00:23:40.790 --> 00:23:47.150
说话人 2: 然后现在的话，我们都已经像现在这样了，可是有一些东西其实你要是想想的话也没有变

202
00:23:47.310 --> 00:23:52.790
说话人 2: 像那个人类简史里面写在7万年前大家就坐一起高 SIP 就是八卦

203
00:23:52.790 --> 00:24:14.790
说话人 2: 对，然后现在的话大家还是会喜欢坐在一起八卦，就是这个人与人之间的关系，然后这个家庭关系，我们的这种同伴的关系，我们的这种社会关系，还有我们所从事的这些事情，我们工作，然后照顾家庭，就这样的一些事情，其实是7万年也没太大变化，我们就周围的整个生活方式变了

204
00:24:14.870 --> 00:24:18.770
说话人 2: 7万年前坐在山洞门口八卦，然后现在我们坐在这个坐

205
00:24:18.810 --> 00:24:20.330
说话人 1: 坐在桌上人行里曝光

206
00:24:20.330 --> 00:24:21.290
说话人 2: 对，就这样的

207
00:24:21.450 --> 00:24:30.930
说话人 2: 所以其实在网友未来展望的话，这个技术一定会把我们的社会生活方式带来很大的变化，但是很可能我们那个时候就坐在一个别的地方八卦在这

208
00:24:31.570 --> 00:24:32.690
说话人 1: 吴老师怎么回答这

209
00:24:32.690 --> 00:24:32.890
说话人 3: 问题

210
00:24:32.890 --> 00:24:43.110
说话人 3: 我觉得是这样子啊，就是说从两个方面看，一个就是说因为科技进步，这个生产出来的物质财富比较多，所以大家基本的生活里都有一个保障

211
00:24:43.150 --> 00:24:49.700
说话人 3: 那么过去我们知道说人均寿命很短，那主要原因就是说吃不饱，这是饥饿，是一个主要的原因

212
00:24:49.700 --> 00:24:56.140
说话人 3: 现在这些都问题不大了，看病慢慢的这些问题也都还 OK 了

213
00:24:56.140 --> 00:25:00.970
说话人 3: 但是从利益点来讲，我就在讲说换一个角度来讲就是人的这个幸福

214
00:25:01.330 --> 00:25:07.050
说话人 3: 今天是否我前一阵写了一篇文章讲就是今天我们的人是否比 18 世纪更幸福

215
00:25:07.410 --> 00:25:16.600
说话人 3: 唉，这个要打一个问号，因为我觉得幸福比如几个主要来源，比如说你爱情和婚姻，你现在这个未必比 18 世纪的人更好

216
00:25:16.600 --> 00:25:24.710
说话人 3: 对，比如说我们，比如你看很简单，你看那简奥斯汀的那些书里头，那个里头那个叫什么傲慢以偏见，艾玛里头人过得很好的

217
00:25:24.790 --> 00:25:33.110
说话人 3: 对，当然你可以说，噢，那是当时的有钱人，但是今天我们的有钱人不一定能过得了他们那样的一个生活

218
00:25:33.450 --> 00:26:11.820
说话人 3: 所以从另一个角度来讲，我觉得说甭管这事业怎么变，有几几，就几件事儿，我一直觉得我们应该人是注意的，就是比如说一个过一个从容的生活，一个优雅的生活人，那个你出门前你觉得你对社会有一个责任感，你尽一个责，然后你实际上明白说我们有些时候人是目的和手段是颠倒了，就是说你比如我们挣了钱是为了过更好的生活，而不是说把自己生活弄得很糟糕，去挣钱这些东西想清楚以后这是人最根本的幸福的一个，越来越

219
00:26:11.860 --> 00:26:19.910
说话人 1: 你像我看他这个人类简史这个作者，他就觉得从生物学的角度说快乐和幸福是个生物化学的问题

220
00:26:20.350 --> 00:26:27.430
说话人 1: 你觉得你找这个女朋友你就幸福，其实更本质的是你这个大脑的一些化学分泌，对，让你感觉幸福

221
00:26:27.550 --> 00:26:35.390
说话人 2: 但是其实这个反过来讲就是化学分泌肯定能影响到我们的思维，但是反过来讲其实我们的思维也能影响到化学氛围

222
00:26:35.710 --> 00:26:36.870
说话人 1: 对，甚至影响遗传

223
00:26:36.950 --> 00:26:37.470
说话人 2: 就是

224
00:26:37.470 --> 00:26:42.910
说话人 2: 对是就是实际上我对你的这个认知本身它会影响到我这化学激素的

225
00:26:42.910 --> 00:26:43.270
说话人 2: 对

226
00:26:43.310 --> 00:26:45.590
说话人 1: 对对，好好好，太好了，太好了
